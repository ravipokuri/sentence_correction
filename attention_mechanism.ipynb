{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "557f375c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d6b1d4ad"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('sms_eng1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "87395383"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('sms_eng1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d040fe01"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, validation = train_test_split(data, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "68d0cb25"
   },
   "outputs": [],
   "source": [
    "train.iloc[0]['english_inp']= str(train.iloc[0]['english_inp'])+' <end>'\n",
    "train.iloc[0]['english_out']= str(train.iloc[0]['english_out'])+' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fa7e727e",
    "outputId": "3a0cb5be-d76d-42e7-d52e-766f9bd7c25f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>english_inp</th>\n",
       "      <th>english_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>hmmm i dunno if she will but i dun thk shell b...</td>\n",
       "      <td>&lt;start&gt; hmm i dont know if she will but i dont...</td>\n",
       "      <td>hmm i dont know if she will but i dont think s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>kenmy hp num is 0165460953</td>\n",
       "      <td>&lt;start&gt; ken my handphone number is 0165460953</td>\n",
       "      <td>ken my handphone number is 0165460953 &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>hey meet at 11 on msn</td>\n",
       "      <td>&lt;start&gt; hey meet at 11 on msn</td>\n",
       "      <td>hey meet at 11 on msn &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>daddy going out tonight  come home urself buy ...</td>\n",
       "      <td>&lt;start&gt; daddy is going out tonight you come ho...</td>\n",
       "      <td>daddy is going out tonight you come home yours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>tt pests fathers hp laden u come faster ah</td>\n",
       "      <td>&lt;start&gt; that pests fathers handphone then you ...</td>\n",
       "      <td>that pests fathers handphone then you come fas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sms  ...                                        english_out\n",
       "629   hmmm i dunno if she will but i dun thk shell b...  ...  hmm i dont know if she will but i dont think s...\n",
       "1774                         kenmy hp num is 0165460953  ...        ken my handphone number is 0165460953 <end>\n",
       "1096                              hey meet at 11 on msn  ...                        hey meet at 11 on msn <end>\n",
       "925   daddy going out tonight  come home urself buy ...  ...  daddy is going out tonight you come home yours...\n",
       "1804         tt pests fathers hp laden u come faster ah  ...  that pests fathers handphone then you come fas...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7f6769d",
    "outputId": "4daa8f48-3d2f-4e17-a443-2612ea05f05b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080\n",
      "4320\n"
     ]
    }
   ],
   "source": [
    "tokenizer_sms = Tokenizer(oov_token=True)\n",
    "tokenizer_sms.fit_on_texts(train[\"sms\"].values)\n",
    "tokenizer_eng=Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_eng.fit_on_texts(train['english_inp'].values)\n",
    "vocab_size_eng=len(tokenizer_eng.word_index.keys())\n",
    "print(vocab_size_eng)\n",
    "vocab_size_sms=len(tokenizer_sms.word_index.keys())\n",
    "print(vocab_size_sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b74b07ad"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drive/MyDrive/glove_vectors', 'rb') as f:\n",
    "    glove = pickle.load(f)\n",
    "    glove_words =  set(glove.keys())\n",
    "from numpy import zeros\n",
    "vocab = len(tokenizer_eng.word_index)+1\n",
    "essay_mat = zeros((vocab, 300))\n",
    "for word, i in tokenizer_eng.word_index.items():\n",
    "    if word in glove_words:\n",
    "        vector = glove[word]\n",
    "        essay_mat[i] = vector\n",
    "vocab1 = len(tokenizer_sms.word_index)+1\n",
    "essay_mat1 = zeros((vocab1, 300))\n",
    "for word, i in tokenizer_sms.word_index.items():\n",
    "    if word in glove_words:\n",
    "        vector = glove[word]\n",
    "        essay_mat1[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "008f311c"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        super().__init__()\n",
    "        self.vocab_size = inp_vocab_size\n",
    "        self.embedding_dim = embedding_size\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= lstm_size\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,trainable=False,weights=[essay_mat1] ,name=\"embedding_layer_encoder\")\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,training=True):\n",
    "        \n",
    "      '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "      '''\n",
    "      input_embedd                           = self.embedding(input_sequence)\n",
    "      self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "      return self.lstm_output, self.lstm_state_h,self.lstm_state_c    \n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      state_h=np.zeros([batch_size,self.enc_units])\n",
    "      state_c=np.zeros([batch_size,self.enc_units])\n",
    "      return state_h,state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dafd8c31"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "\n",
    "    super().__init__()\n",
    "    self.scoring_function=scoring_function\n",
    "    self.au=att_units\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      self.sm=tf.keras.layers.Softmax(axis=1)\n",
    "      \n",
    "    if scoring_function == 'general':\n",
    "      # Intialize variables needed for General score function here\n",
    "      self.d=Dense(self.au)\n",
    "      self.sm=tf.keras.layers.Softmax(axis=1)\n",
    "      \n",
    "    elif scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "      self.sm=tf.keras.layers.Softmax(axis=1)\n",
    "      self.d1=Dense(self.au)\n",
    "      self.d2=Dense(1)\n",
    "      self.d3=Dense(self.au)\n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    \n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        decoder_hidden_state=tf.keras.layers.Reshape((decoder_hidden_state.shape[1],1))(decoder_hidden_state)\n",
    "        dot_product=tf.keras.layers.Dot(axes=(2,1))([encoder_output,decoder_hidden_state])\n",
    "        weight=self.sm(dot_product)\n",
    "        con_vec=tf.keras.layers.Dot(axes=(1,1))([weight,encoder_output])\n",
    "        con_vec=tf.math.reduce_sum(con_vec,axis=1)\n",
    "        return con_vec,weight\n",
    "    elif self.scoring_function == 'general':\n",
    "        # Implement General score function here\n",
    "        decoder_hidden_state=tf.keras.layers.Reshape((decoder_hidden_state.shape[1],1))(decoder_hidden_state)\n",
    "        d1=tf.keras.layers.Dot(axes=(2,1))([encoder_output,decoder_hidden_state])\n",
    "        weight=self.sm(d1)\n",
    "        con_vec=tf.keras.layers.Dot(axes=(1,1))([weight,encoder_output])\n",
    "        con_vec=tf.math.reduce_sum(con_vec,axis=1)\n",
    "        return con_vec,weight\n",
    "    elif self.scoring_function == 'concat':\n",
    "        # Implement General score function here\n",
    "        temp=self.d2(tf.keras.activations.tanh(self.d1(encoder_output)+self.d3(tf.expand_dims(decoder_hidden_state,1))))\n",
    "        weight=self.sm(temp)\n",
    "        con_vec=tf.keras.layers.Dot(axes=(1,1))([weight,encoder_output])\n",
    "        con_vec=tf.math.reduce_sum(con_vec,axis=1)\n",
    "        return con_vec,weight\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "00fa78c7"
   },
   "outputs": [],
   "source": [
    "class OneStepDecoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      super().__init__()\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      self.vocab_size=tar_vocab_size\n",
    "      self.embedding_size=embedding_dim\n",
    "      self.input_length=input_length\n",
    "      self.dec_units=dec_units\n",
    "      self.score_fun=score_fun\n",
    "      self.au=att_units\n",
    "      self.embedding=Embedding(input_dim=self.vocab_size,output_dim=300,input_length=self.input_length,trainable=False,weights=[essay_mat],mask_zero=True,name='Embedding_layer_decoder')\n",
    "      self.lstm_layer=LSTM(units=self.dec_units,return_sequences=True,return_state=True,name='Lstm_decoder')\n",
    "      self.dense_layer=Dense(self.vocab_size)\n",
    "      self.attention_layer=Attention(self.score_fun,self.au)\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "\n",
    "    con_vec,weight=self.attention_layer(state_h,encoder_output)\n",
    "    target_embedding=self.embedding(input_to_decoder)\n",
    "    temp=tf.expand_dims(con_vec,1)\n",
    "    concat=tf.concat([target_embedding,temp],axis=2)\n",
    "    out,hidden,cell=self.lstm_layer(concat)\n",
    "    out = tf.reshape(out, (-1, out.shape[2]))\n",
    "    out=self.dense_layer(out)      \n",
    "    return out,hidden,cell,weight,con_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4a4d483a"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      super().__init__()\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      self.vocab_size=out_vocab_size\n",
    "      self.embedding_dim=embedding_dim\n",
    "      self.input_length=input_length\n",
    "      self.dec_units=dec_units\n",
    "      self.score_fun=score_fun\n",
    "      self.au=att_units\n",
    "      self.osd=OneStepDecoder(self.vocab_size,self.embedding_dim,self.input_length,self.dec_units,self.score_fun,self.au)\n",
    "\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        \n",
    "        temp = tf.TensorArray(tf.float32,size=len(input_to_decoder[0]),name='empty_tensor')\n",
    "        for j in range(len(input_to_decoder[0])):\n",
    "          out_one, hidden, cell,weight,con_vec = self.osd(input_to_decoder[:,j:j+1],encoder_output,decoder_hidden_state,decoder_cell_state)\n",
    "          temp=temp.write(j,out_one)\n",
    "        temp=tf.transpose(temp.stack(),[1,0,2])\n",
    "        return temp\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "126e8375"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,int_len,out_len,  score_fun, att_units,batch_size):\n",
    "    super().__init__()\n",
    "    #Intialize objects from encoder decoder\n",
    "    self.int_len=int_len\n",
    "    self.out_len=out_len\n",
    "    self.score=score_fun\n",
    "    self.au=att_units\n",
    "    self.batch_size=batch_size\n",
    "    self.encoder = Encoder(inp_vocab_size=vocab_size_sms+1, embedding_size=300, input_length=self.int_len, lstm_size=256)\n",
    "    self.decoder = Decoder(out_vocab_size=vocab_size_eng+1, embedding_dim=300,input_length=self.out_len,dec_units=256,score_fun=self.score,att_units=self.au)\n",
    "    self.encoder_state_h,self.encoder_state_c=self.encoder.initialize_states(self.batch_size)\n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    encoder_output, encoder_hidden, encoder_cell = self.encoder(data[0],[self.encoder_state_h,self.encoder_state_c])\n",
    "    output                     = self.decoder(data[1],encoder_output,encoder_hidden,encoder_cell)\n",
    "    return output \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e78d8c44"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tokenizer_sms, tokenizer_eng, max_len):\n",
    "        self.encoder_inps = data['sms'].values\n",
    "        self.decoder_inps = data['english_inp'].values\n",
    "        self.decoder_outs = data['english_out'].values\n",
    "        self.tokenizer_eng = tokenizer_eng\n",
    "        self.tokenizer_sms = tokenizer_sms\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tokenizer_sms.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tokenizer_eng.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tokenizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data_get = []\n",
    "        for j in range(start, stop):\n",
    "            data_get.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1),axis=0) for samples in zip(*data_get)]\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5030c012",
    "outputId": "18523e46-a003-4b59-98d1-d0710468a449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60) (64, 60) (64, 60)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tokenizer_sms, tokenizer_eng, 60)\n",
    "test_dataset  = Dataset(validation, tokenizer_sms, tokenizer_eng, 60)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=64)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bbc1d17",
    "outputId": "15227473-2cfc-4889-b987-89b671dd4a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/150\n",
      "30/30 [==============================] - 25s 848ms/step - loss: 9.3364\n",
      "Epoch 2/150\n",
      "30/30 [==============================] - 25s 830ms/step - loss: 8.4376\n",
      "Epoch 3/150\n",
      "30/30 [==============================] - 24s 815ms/step - loss: 8.4019\n",
      "Epoch 4/150\n",
      "30/30 [==============================] - 24s 802ms/step - loss: 8.3863\n",
      "Epoch 5/150\n",
      "30/30 [==============================] - 24s 804ms/step - loss: 8.3982\n",
      "Epoch 6/150\n",
      "30/30 [==============================] - 23s 781ms/step - loss: 8.3653\n",
      "Epoch 7/150\n",
      "30/30 [==============================] - 24s 792ms/step - loss: 8.3643\n",
      "Epoch 8/150\n",
      "30/30 [==============================] - 24s 800ms/step - loss: 8.3634\n",
      "Epoch 9/150\n",
      "30/30 [==============================] - 24s 803ms/step - loss: 8.3324\n",
      "Epoch 10/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.3372\n",
      "Epoch 11/150\n",
      "30/30 [==============================] - 24s 806ms/step - loss: 8.3628\n",
      "Epoch 12/150\n",
      "30/30 [==============================] - 24s 813ms/step - loss: 8.3285\n",
      "Epoch 13/150\n",
      "30/30 [==============================] - 24s 799ms/step - loss: 8.3080\n",
      "Epoch 14/150\n",
      "30/30 [==============================] - 24s 806ms/step - loss: 8.3005\n",
      "Epoch 15/150\n",
      "30/30 [==============================] - 24s 804ms/step - loss: 8.2723\n",
      "Epoch 16/150\n",
      "30/30 [==============================] - 25s 818ms/step - loss: 8.2673\n",
      "Epoch 17/150\n",
      "30/30 [==============================] - 24s 810ms/step - loss: 8.2825\n",
      "Epoch 18/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.2583\n",
      "Epoch 19/150\n",
      "30/30 [==============================] - 24s 798ms/step - loss: 8.2579\n",
      "Epoch 20/150\n",
      "30/30 [==============================] - 24s 805ms/step - loss: 8.2339\n",
      "Epoch 21/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.2169\n",
      "Epoch 22/150\n",
      "30/30 [==============================] - 24s 809ms/step - loss: 8.2857\n",
      "Epoch 23/150\n",
      "30/30 [==============================] - 24s 807ms/step - loss: 8.3657\n",
      "Epoch 24/150\n",
      "30/30 [==============================] - 23s 780ms/step - loss: 8.3248\n",
      "Epoch 25/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.3273\n",
      "Epoch 26/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.3262\n",
      "Epoch 27/150\n",
      "30/30 [==============================] - 24s 807ms/step - loss: 8.2873\n",
      "Epoch 28/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.2677\n",
      "Epoch 29/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.3244\n",
      "Epoch 30/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.2216\n",
      "Epoch 31/150\n",
      "30/30 [==============================] - 23s 766ms/step - loss: 8.2233\n",
      "Epoch 32/150\n",
      "30/30 [==============================] - 24s 787ms/step - loss: 8.2456\n",
      "Epoch 33/150\n",
      "30/30 [==============================] - 24s 801ms/step - loss: 8.2726\n",
      "Epoch 34/150\n",
      "30/30 [==============================] - 24s 806ms/step - loss: 8.2743\n",
      "Epoch 35/150\n",
      "30/30 [==============================] - 24s 797ms/step - loss: 8.2546\n",
      "Epoch 36/150\n",
      "30/30 [==============================] - 24s 785ms/step - loss: 8.1161\n",
      "Epoch 37/150\n",
      "30/30 [==============================] - 24s 791ms/step - loss: 8.0311\n",
      "Epoch 38/150\n",
      "30/30 [==============================] - 24s 799ms/step - loss: 8.0310\n",
      "Epoch 39/150\n",
      "30/30 [==============================] - 23s 779ms/step - loss: 8.0310\n",
      "Epoch 40/150\n",
      "30/30 [==============================] - 24s 810ms/step - loss: 8.0310\n",
      "Epoch 41/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.0310\n",
      "Epoch 42/150\n",
      "30/30 [==============================] - 23s 779ms/step - loss: 8.0310\n",
      "Epoch 43/150\n",
      "30/30 [==============================] - 24s 791ms/step - loss: 8.0310\n",
      "Epoch 44/150\n",
      "30/30 [==============================] - 24s 798ms/step - loss: 8.0310\n",
      "Epoch 45/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.0310\n",
      "Epoch 46/150\n",
      "30/30 [==============================] - 24s 805ms/step - loss: 8.0310\n",
      "Epoch 47/150\n",
      "30/30 [==============================] - 25s 821ms/step - loss: 8.0310\n",
      "Epoch 48/150\n",
      "30/30 [==============================] - 24s 810ms/step - loss: 8.0310\n",
      "Epoch 49/150\n",
      "30/30 [==============================] - 23s 781ms/step - loss: 8.0310\n",
      "Epoch 50/150\n",
      "30/30 [==============================] - 24s 808ms/step - loss: 8.0310\n",
      "Epoch 51/150\n",
      "30/30 [==============================] - 24s 791ms/step - loss: 8.0310\n",
      "Epoch 52/150\n",
      "30/30 [==============================] - 24s 796ms/step - loss: 8.0310\n",
      "Epoch 53/150\n",
      "30/30 [==============================] - 23s 769ms/step - loss: 8.0310\n",
      "Epoch 54/150\n",
      "30/30 [==============================] - 24s 798ms/step - loss: 8.0310\n",
      "Epoch 55/150\n",
      "30/30 [==============================] - 23s 763ms/step - loss: 8.0310\n",
      "Epoch 56/150\n",
      "30/30 [==============================] - 23s 777ms/step - loss: 8.0310\n",
      "Epoch 57/150\n",
      "30/30 [==============================] - 24s 802ms/step - loss: 8.0310\n",
      "Epoch 58/150\n",
      "30/30 [==============================] - 23s 781ms/step - loss: 8.0310\n",
      "Epoch 59/150\n",
      "30/30 [==============================] - 23s 768ms/step - loss: 8.0310\n",
      "Epoch 60/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 61/150\n",
      "30/30 [==============================] - 24s 792ms/step - loss: 8.0310\n",
      "Epoch 62/150\n",
      "30/30 [==============================] - 24s 785ms/step - loss: 8.0310\n",
      "Epoch 63/150\n",
      "30/30 [==============================] - 24s 787ms/step - loss: 8.0310\n",
      "Epoch 64/150\n",
      "30/30 [==============================] - 25s 819ms/step - loss: 8.0310\n",
      "Epoch 65/150\n",
      "30/30 [==============================] - 24s 799ms/step - loss: 8.0310\n",
      "Epoch 66/150\n",
      "30/30 [==============================] - 24s 795ms/step - loss: 8.0310\n",
      "Epoch 67/150\n",
      "30/30 [==============================] - 25s 819ms/step - loss: 8.0310\n",
      "Epoch 68/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.0310\n",
      "Epoch 69/150\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 8.0310\n",
      "Epoch 70/150\n",
      "30/30 [==============================] - 24s 801ms/step - loss: 8.0310\n",
      "Epoch 71/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 72/150\n",
      "30/30 [==============================] - 24s 784ms/step - loss: 8.0310\n",
      "Epoch 73/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 74/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 75/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 76/150\n",
      "30/30 [==============================] - 23s 774ms/step - loss: 8.0310\n",
      "Epoch 77/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 78/150\n",
      "30/30 [==============================] - 24s 792ms/step - loss: 8.0310\n",
      "Epoch 79/150\n",
      "30/30 [==============================] - 23s 770ms/step - loss: 8.0310\n",
      "Epoch 80/150\n",
      "30/30 [==============================] - 24s 791ms/step - loss: 8.0310\n",
      "Epoch 81/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.0310\n",
      "Epoch 82/150\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 8.0310\n",
      "Epoch 83/150\n",
      "30/30 [==============================] - 24s 801ms/step - loss: 8.0310\n",
      "Epoch 84/150\n",
      "30/30 [==============================] - 24s 785ms/step - loss: 8.0310\n",
      "Epoch 85/150\n",
      "30/30 [==============================] - 24s 795ms/step - loss: 8.0310\n",
      "Epoch 86/150\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 8.0310\n",
      "Epoch 87/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 88/150\n",
      "30/30 [==============================] - 23s 755ms/step - loss: 8.0310\n",
      "Epoch 89/150\n",
      "30/30 [==============================] - 23s 777ms/step - loss: 8.0310\n",
      "Epoch 90/150\n",
      "30/30 [==============================] - 23s 754ms/step - loss: 8.0310\n",
      "Epoch 91/150\n",
      "30/30 [==============================] - 23s 780ms/step - loss: 8.0310\n",
      "Epoch 92/150\n",
      "30/30 [==============================] - 23s 768ms/step - loss: 8.0310\n",
      "Epoch 93/150\n",
      "30/30 [==============================] - 24s 786ms/step - loss: 8.0310\n",
      "Epoch 94/150\n",
      "30/30 [==============================] - 24s 788ms/step - loss: 8.0310\n",
      "Epoch 95/150\n",
      "30/30 [==============================] - 23s 768ms/step - loss: 8.0310\n",
      "Epoch 96/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.0310\n",
      "Epoch 97/150\n",
      "30/30 [==============================] - 23s 763ms/step - loss: 8.0310\n",
      "Epoch 98/150\n",
      "30/30 [==============================] - 23s 776ms/step - loss: 8.0310\n",
      "Epoch 99/150\n",
      "30/30 [==============================] - 23s 779ms/step - loss: 8.0310\n",
      "Epoch 100/150\n",
      "30/30 [==============================] - 23s 763ms/step - loss: 8.0310\n",
      "Epoch 101/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 102/150\n",
      "30/30 [==============================] - 24s 784ms/step - loss: 8.0310\n",
      "Epoch 103/150\n",
      "30/30 [==============================] - 23s 775ms/step - loss: 8.0310\n",
      "Epoch 104/150\n",
      "30/30 [==============================] - 23s 767ms/step - loss: 8.0310\n",
      "Epoch 105/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.0310\n",
      "Epoch 106/150\n",
      "30/30 [==============================] - 24s 786ms/step - loss: 8.0310\n",
      "Epoch 107/150\n",
      "30/30 [==============================] - 24s 787ms/step - loss: 8.0310\n",
      "Epoch 108/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 109/150\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 8.0310\n",
      "Epoch 110/150\n",
      "30/30 [==============================] - 24s 806ms/step - loss: 8.0310\n",
      "Epoch 111/150\n",
      "30/30 [==============================] - 24s 810ms/step - loss: 8.0310\n",
      "Epoch 112/150\n",
      "30/30 [==============================] - 24s 805ms/step - loss: 8.0310\n",
      "Epoch 113/150\n",
      "30/30 [==============================] - 23s 780ms/step - loss: 8.0310\n",
      "Epoch 114/150\n",
      "30/30 [==============================] - 24s 803ms/step - loss: 8.0310\n",
      "Epoch 115/150\n",
      "30/30 [==============================] - 24s 787ms/step - loss: 8.0310\n",
      "Epoch 116/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.0310\n",
      "Epoch 117/150\n",
      "30/30 [==============================] - 24s 800ms/step - loss: 8.0310\n",
      "Epoch 118/150\n",
      "30/30 [==============================] - 23s 782ms/step - loss: 8.0310\n",
      "Epoch 119/150\n",
      "30/30 [==============================] - 24s 795ms/step - loss: 8.0310\n",
      "Epoch 120/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 121/150\n",
      "30/30 [==============================] - 23s 775ms/step - loss: 8.0310\n",
      "Epoch 122/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 123/150\n",
      "30/30 [==============================] - 23s 776ms/step - loss: 8.0310\n",
      "Epoch 124/150\n",
      "30/30 [==============================] - 24s 798ms/step - loss: 8.0310\n",
      "Epoch 125/150\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 8.0310\n",
      "Epoch 126/150\n",
      "30/30 [==============================] - 24s 789ms/step - loss: 8.0310\n",
      "Epoch 127/150\n",
      "30/30 [==============================] - 23s 772ms/step - loss: 8.0310\n",
      "Epoch 128/150\n",
      "30/30 [==============================] - 24s 809ms/step - loss: 8.0310\n",
      "Epoch 129/150\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 8.0310\n",
      "Epoch 130/150\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 8.0310\n",
      "Epoch 131/150\n",
      "30/30 [==============================] - 24s 793ms/step - loss: 8.0310\n",
      "Epoch 132/150\n",
      "30/30 [==============================] - 23s 757ms/step - loss: 8.0310\n",
      "Epoch 133/150\n",
      "30/30 [==============================] - 24s 814ms/step - loss: 8.0310\n",
      "Epoch 134/150\n",
      "30/30 [==============================] - 23s 767ms/step - loss: 8.0310\n",
      "Epoch 135/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 136/150\n",
      "30/30 [==============================] - 23s 775ms/step - loss: 8.0310\n",
      "Epoch 137/150\n",
      "30/30 [==============================] - 24s 794ms/step - loss: 8.0310\n",
      "Epoch 138/150\n",
      "30/30 [==============================] - 24s 788ms/step - loss: 8.0310\n",
      "Epoch 139/150\n",
      "30/30 [==============================] - 23s 773ms/step - loss: 8.0310\n",
      "Epoch 140/150\n",
      "30/30 [==============================] - 22s 750ms/step - loss: 8.0310\n",
      "Epoch 141/150\n",
      "30/30 [==============================] - 23s 770ms/step - loss: 8.0310\n",
      "Epoch 142/150\n",
      "30/30 [==============================] - 24s 803ms/step - loss: 8.0310\n",
      "Epoch 143/150\n",
      "30/30 [==============================] - 23s 778ms/step - loss: 8.0310\n",
      "Epoch 144/150\n",
      "30/30 [==============================] - 23s 773ms/step - loss: 8.0310\n",
      "Epoch 145/150\n",
      "30/30 [==============================] - 23s 783ms/step - loss: 8.0310\n",
      "Epoch 146/150\n",
      "30/30 [==============================] - 24s 785ms/step - loss: 8.0310\n",
      "Epoch 147/150\n",
      "30/30 [==============================] - 23s 771ms/step - loss: 8.0310\n",
      "Epoch 148/150\n",
      "30/30 [==============================] - 24s 790ms/step - loss: 8.0310\n",
      "Epoch 149/150\n",
      "30/30 [==============================] - 23s 758ms/step - loss: 8.0310\n",
      "Epoch 150/150\n",
      "30/30 [==============================] - 23s 764ms/step - loss: 8.0310\n",
      "Model: \"encoder_decoder_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_2 (Encoder)          multiple                  1862768   \n",
      "_________________________________________________________________\n",
      "decoder_2 (Decoder)          multiple                  2545287   \n",
      "=================================================================\n",
      "Total params: 4,408,055\n",
      "Trainable params: 2,193,155\n",
      "Non-trainable params: 2,214,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "log_dir=\"log/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "#Create an object of encoder_decoder Model class, \n",
    "# Compile the model and fit the model\n",
    "model  = encoder_decoder(int_len=60,out_len=60,score_fun='dot',att_units=64,batch_size=64)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
    "train_steps=train.shape[0]//64\n",
    "valid_steps=validation.shape[0]//64\n",
    "model.fit(train_dataloader, steps_per_epoch=train_steps, epochs=150, validation_data=test_dataloader, validation_steps=valid_steps,callbacks=[tensorboard])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "834e1ed3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_sentence,scoring_fun):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "  E. Call plot_attention(#params)\n",
    "  F. Return the predicted sentence\n",
    "  '''\n",
    "  encoder_sms=tokenizer_sms.texts_to_sequences([input_sentence])\n",
    "  encoder_pad=pad_sequences(encoder_sms,maxlen=60,padding='post',dtype='int32')\n",
    "  encoder_out,encoder_hidden,encoder_cell=model.layers[0](encoder_pad)\n",
    "  index=tokenizer_eng.word_index['<start>']\n",
    "  index=np.reshape(index,(1,1))\n",
    "  out=[]\n",
    "  for j in range(60):\n",
    "    output, h, c, weight, con_vec=model.layers[1].osd(index, encoder_out, encoder_hidden, encoder_cell)\n",
    "    decoder_out=model.layers[1](index,encoder_out,encoder_hidden,encoder_cell)\n",
    "    prob=np.argmax(decoder_out)\n",
    "    index=np.reshape(prob,(1,1))\n",
    "    if prob!=0:\n",
    "        out.append(tokenizer_eng.index_word[prob])\n",
    "    if prob!=0:\n",
    "        if tokenizer_eng.index_word[prob]=='<end>':\n",
    "            break\n",
    "  return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "b3afc315"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e6551ed",
    "outputId": "e81c8c6d-88a7-4b81-fbc8-3ba42f68ae6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "sms: wanna come sit with us at right row 23 corner\n",
      "actual: do you want to come and sit with us at right row 23 corner <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: help me find millians mem card thk its on e dining table\n",
      "actual: help me find millians mem card think its on the dining table <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: she alone lah muz b w somebody meh\n",
      "actual: she is alone she must be with somebody <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: hello hows ur day oh ya wana tell u tt ill b workin until nxt wed onlyhaha takin a break\n",
      "actual: hello how is your day oh ya want to tell you that i will be working until next wednesday only haha taking a break <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: haha ya i don mind u are not right\n",
      "actual: haha i dont mind you are not right <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: ok when is e interview u need more pple my fren interested oso\n",
      "actual: ok when is the interview you need more people my friend is interested also <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: so sad i bought e opera bar got no discount so small somemore\n",
      "actual: so sad i bought the opera bar without any discount and it is so small <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: hmmm ya i don mind goin but she sounds like she really needs to sell e ticketsare u goin\n",
      "actual: hm yes i dont mind going but she sounds like she really needs to sell the tickets are you going <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: huh can like that meh bishan already was eating haha\n",
      "actual: can it be like that bishan already was eating haha <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: im going to mom to submit doc if u want to go outwe can meet me outside\n",
      "actual: im going to mom to submit document if you want to go out we can meet me outside <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: i got tell  before ma got one lady keep stalkin my father she called today den i wuz like quarrellin wit her on de fone anywae nite tc\n",
      "actual: i got tell you before got one lady who keeps stalking my father she called today then i was like quarrelling with her on the phone anyway good night take care <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: christ want to chat anot\n",
      "actual: christ wants to chat or not <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: oh i can take a bus there 103 right\n",
      "actual: oh i can take a bus there it is 103 right <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: hey sentosa camp laguna still got tentsonli 24 4 a 8man tent per nite so if itz okay ill bk 2 tents on 7n8jun n oso2 bbq pits ya\n",
      "actual: hey sentosa laguna camp still got tents only 24 for an 8 man tent per night so if its okay ill book 2 tents on 7 and 8 june and also 2 barbeque pits okay <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: haha im carrying a broom with me so really paiseh to walk into lecture with it im coming straight from home mah cya later then\n",
      "actual: haha im carrying a broom with me so im really sorry to walk into lecture with it im coming straight from home see you later then <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: hey wanna go ecp tmr after airport\n",
      "actual: hey want to go to ecp tomorrow after airport <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: wah today rainin cats n dogs u wan me go running 2 days ago run 10 k liao heheh but fire burn out wat time u flyin\n",
      "actual: its raining cats and dogs today and you want me to run i have already ran 10k 2 days ago but the fire burnt out what time are you flying <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: huh den help me bring my ic in e pink tray in my rm ok den u wan go ntuc at e same time anot\n",
      "actual: huh then help me bring my ic in the pink tray in my room okay then you want to go ntuc at the same time or not <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: im goin orchard nowbut gota go home for dinnerbut can pei  eathee\n",
      "actual: im going orchard now but i need to go home for dinner but i can accompany you to eat hee <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "305\n",
      "sms: both no nd com eng stuff thats what my bro replied haha i think your fren will be disappointed\n",
      "actual: both no need com eng stuff thats what my brother replied haha i think your friend will be disappointed <end>\n",
      "prediction: difference head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head head\n",
      "bleu_score: 7.57557535389896e-232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.translate import bleu_score\n",
    "sms_inp=validation['sms'].values[:20]\n",
    "eng_inp=validation['english_out'].values[:20]\n",
    "score=[]\n",
    "for i in range(20):\n",
    "    prediction=predict(sms_inp[i],'dot')\n",
    "    print(len(prediction))\n",
    "    print('sms:',sms_inp[i])\n",
    "    print('actual:',eng_inp[i])\n",
    "    print('prediction:',prediction)\n",
    "    score.append(bleu_score.sentence_bleu(eng_inp[i],prediction))\n",
    "print('bleu_score:',np.average(score))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "attention_mechanism.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
